{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/q89771441/AML_little_project/blob/main/Machine_Learning_2025_Spring_HW7.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fen_TSKDjrbL"
      },
      "source": [
        "### Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "6GiDl1kZjrbL"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "import os\n",
        "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
        "    !pip install unsloth==2025.3.19 vllm\n",
        "else:\n",
        "    # [NOTE] Do the below ONLY in Colab! Use [[pip install unsloth vllm]]\n",
        "    !pip install --no-deps unsloth==2025.3.19 vllm\n",
        "!pip install --no-deps transformers==4.50.3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O17a_hhJ0n14"
      },
      "outputs": [],
      "source": [
        "#@title Colab Extra Install { display-mode: \"form\" }\n",
        "%%capture\n",
        "import os\n",
        "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
        "    !pip install unsloth vllm\n",
        "else:\n",
        "    !pip install --no-deps unsloth vllm\n",
        "    # [NOTE] Do the below ONLY in Colab! Use [[pip install unsloth vllm]]\n",
        "    # Skip restarting message in Colab\n",
        "    import sys, re, requests; modules = list(sys.modules.keys())\n",
        "    for x in modules: sys.modules.pop(x) if \"PIL\" in x or \"google\" in x else None\n",
        "    !pip install --no-deps bitsandbytes accelerate xformers==0.0.29.post3 peft \"trl==0.15.2\" triton cut_cross_entropy unsloth_zoo==2025.3.17\n",
        "    !pip install sentencepiece protobuf datasets huggingface_hub hf_transfer\n",
        "\n",
        "    # vLLM requirements - vLLM breaks Colab due to reinstalling numpy\n",
        "    f = requests.get(\"https://raw.githubusercontent.com/vllm-project/vllm/refs/heads/main/requirements/common.txt\").content\n",
        "    with open(\"vllm_requirements.txt\", \"wb\") as file:\n",
        "        file.write(re.sub(rb\"(transformers|numpy|xformers)[^\\n]{1,}\\n\", b\"\", f))\n",
        "    !pip install -r vllm_requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tnZ8QsCVjrbM"
      },
      "source": [
        "### Unsloth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E8-BWi7MzkRz"
      },
      "outputs": [],
      "source": [
        "from unsloth import PatchDPOTrainer\n",
        "\n",
        "PatchDPOTrainer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QmUBVEnvCDJv"
      },
      "outputs": [],
      "source": [
        "from unsloth import FastLanguageModel\n",
        "\n",
        "max_seq_length = 512\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"unsloth/llama-3-8b-Instruct\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    load_in_4bit = True,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EQ-Cp2V6kDcr"
      },
      "source": [
        "<a name=\"Data\"></a>\n",
        "### Data Prep"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6WpAAv6dSBBs"
      },
      "source": [
        "We first download the data files."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XhWYmb2f8mLf"
      },
      "outputs": [],
      "source": [
        "!git clone https://gitlab.com/lchengtw/ML2025Spring-HW7.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9_TGfeWI2mct"
      },
      "source": [
        "Then, we load the json file here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8YFR6sND0Uf6"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "with open(\"/content/ML2025Spring-HW7/train.json\", 'r') as jsonfile:\n",
        "    full_data = json.load(jsonfile)\n",
        "\n",
        "with open(\"/content/ML2025Spring-HW7/test.json\", 'r') as jsonfile:\n",
        "    test_data = json.load(jsonfile)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5f5oZaRm2rOt"
      },
      "source": [
        "We define how we prepare the messages for the model and how we extract the response from the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r6bUnxe6N3pf"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "def data_formulate(data):\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": \"Your entire response must be 100 characters or less.\"},\n",
        "        {\"role\": \"user\", \"content\": data['prompt']},\n",
        "    ]\n",
        "    prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "    return prompt\n",
        "\n",
        "def extract_assistant_response(text):\n",
        "    try:\n",
        "        # Split by assistant header marker\n",
        "        parts = text.split(\"<|start_header_id|>assistant<|end_header_id|>\")\n",
        "        if len(parts) < 2:\n",
        "            return None\n",
        "\n",
        "        # Split by end of text marker\n",
        "        assistant_part = parts[1]\n",
        "        response_parts = assistant_part.split(\"<|eot_id|>\")\n",
        "\n",
        "        # Clean up any whitespace\n",
        "        return response_parts[0].strip()\n",
        "    except Exception as e:\n",
        "        print(f\"Error extracting assistant response: {e}\")\n",
        "        return None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mv_nzF9g26Be"
      },
      "source": [
        "Let's observe how the model responses before aligning it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yyRWEvnT49Op"
      },
      "outputs": [],
      "source": [
        "original_model_response = []\n",
        "for data in test_data:\n",
        "    id = data['id']\n",
        "    prompt = data['prompt']\n",
        "    print(f'\\nQuestion {id}: {prompt}')\n",
        "    inputs = data_formulate(data)\n",
        "    outputs = model.generate(\n",
        "        **tokenizer(inputs, return_tensors = \"pt\").to(\"cuda\"),\n",
        "        max_new_tokens = 128,\n",
        "        do_sample=False\n",
        "    )\n",
        "    output = tokenizer.batch_decode(outputs)[0]\n",
        "    output = extract_assistant_response(output)\n",
        "    original_model_response.append(output)\n",
        "    print()\n",
        "    print(output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oNXnEU9P0-Yf"
      },
      "source": [
        "Now we preapre the data for aligning.\n",
        "\n",
        "Please adjust the parameters here to complete the observations for the assignment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UOUJFkYd97Dk"
      },
      "outputs": [],
      "source": [
        "# TODO: Adjust the parameters here\n",
        "num_epoch = 3\n",
        "data_size = 50\n",
        "support_ratio = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kkOsyv6G98wM"
      },
      "outputs": [],
      "source": [
        "#### DO NOT CHANGE ####\n",
        "\n",
        "from datasets import Dataset\n",
        "\n",
        "# Select part of the data for training\n",
        "training_data = full_data[:data_size]\n",
        "\n",
        "# Define the size of the support dataset\n",
        "support_data_size = int(data_size * support_ratio)\n",
        "\n",
        "# Prepare the data for the training dataset\n",
        "prompt_list = [data_formulate(data) for data in training_data]\n",
        "chosen_list = [data['support'] for data in training_data[:support_data_size]] + [data['oppose'] for data in training_data[support_data_size:]]\n",
        "rejected_list = [data['oppose'] for data in training_data[:support_data_size]] + [data['support'] for data in training_data[support_data_size:]]\n",
        "\n",
        "# Create the training dataset\n",
        "train_dataset = Dataset.from_dict({'prompt': prompt_list, 'chosen': chosen_list, 'rejected': rejected_list})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9fWGW_zC3OFI"
      },
      "source": [
        "Now let's take a look on an example of the prompt, the chosen response and the rejected response."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m2t5R8TfUlNw"
      },
      "outputs": [],
      "source": [
        "prompt_list[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2dTshtFzUoHh"
      },
      "outputs": [],
      "source": [
        "chosen_list[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "58J1VvuZUp7W"
      },
      "outputs": [],
      "source": [
        "rejected_list[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "86wyNoeMj-Ph"
      },
      "source": [
        "We now add LoRA adapters so we only need to update 1 to 10% of all parameters.\n",
        "\n",
        "Please do not change anything here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6bZsfBuZDeCL"
      },
      "outputs": [],
      "source": [
        "#### DO NOT CHANGE ####\n",
        "\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
        "\n",
        "    r = 16,           # Larger = higher accuracy, but might overfit\n",
        "    lora_alpha = 16,  # Recommended alpha == r at least\n",
        "    lora_dropout = 0.1,\n",
        "    bias = \"none\",\n",
        "    random_state = 3407, # Do not modify the random_state for reproducibility\n",
        "    use_rslora = False,  # We support rank stabilized LoRA\n",
        "    loftq_config = None, # And LoftQ\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-kyd_iyz7DUM"
      },
      "source": [
        "<a name=\"Train\"></a>\n",
        "### Train the DPO model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BcOJUdRf3jFk"
      },
      "source": [
        "Now we define the trainer.\n",
        "\n",
        "Please (also) do not change anything here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QtoqUw80QDV0"
      },
      "outputs": [],
      "source": [
        "#### DO NOT CHANGE ####\n",
        "\n",
        "from transformers import TrainingArguments\n",
        "from trl import DPOTrainer, DPOConfig\n",
        "from unsloth import is_bfloat16_supported\n",
        "\n",
        "dpo_trainer = DPOTrainer(\n",
        "    model = model,\n",
        "    ref_model = None,\n",
        "    args = DPOConfig(\n",
        "        per_device_train_batch_size = 2,\n",
        "        gradient_accumulation_steps = 4,\n",
        "        warmup_ratio = 0.1,\n",
        "        num_train_epochs = num_epoch,\n",
        "        learning_rate = 1e-4,\n",
        "        fp16 = not is_bfloat16_supported(),\n",
        "        bf16 = is_bfloat16_supported(),\n",
        "        logging_steps = 1,\n",
        "        optim = \"paged_adamw_8bit\",\n",
        "        weight_decay = 0.0,\n",
        "        lr_scheduler_type = \"linear\",\n",
        "        seed = 42,\n",
        "        output_dir = \"outputs\",\n",
        "        report_to = \"none\",\n",
        "    ),\n",
        "    beta = 0.1,\n",
        "    train_dataset = train_dataset,\n",
        "    tokenizer = tokenizer,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sPnHWfyj3taW"
      },
      "source": [
        "Now we start training!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EWGFqAo5Q2me"
      },
      "outputs": [],
      "source": [
        "dpo_trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tCiUsutb3x_N"
      },
      "source": [
        "After training, we utilize the model to do the inference on the test again to see how it differs from the original model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WCpsag8B-OVL"
      },
      "outputs": [],
      "source": [
        "aligned_model_response = []\n",
        "for data in test_data:\n",
        "    id = data['id']\n",
        "    prompt = data['prompt']\n",
        "    print(f'\\nQuestion {id}: {prompt}')\n",
        "    inputs = data_formulate(data)\n",
        "    outputs = model.generate(\n",
        "        **tokenizer(inputs, return_tensors = \"pt\").to(\"cuda\"),\n",
        "        max_new_tokens = 128,\n",
        "        do_sample=False\n",
        "    )\n",
        "    output = tokenizer.batch_decode(outputs)[0]\n",
        "    output = extract_assistant_response(output)\n",
        "    aligned_model_response.append(output)\n",
        "    print()\n",
        "    print(output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A92QwKl0ImK1"
      },
      "source": [
        "Next, we save the results in .json for your NTU COOL submission.\n",
        "\n",
        "Please note that this is designed for Colab, you may have to change the directory name for other machines."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gtfGrYFGEL_-"
      },
      "outputs": [],
      "source": [
        "student_id = \"B12345678\" # TODO: fill in your student id here.\n",
        "dir_name = \"/content\" # TODO: If you use machines other than colab, please adjust the directory here.\n",
        "# Do NOT change the following for this block.\n",
        "file_name = f\"{dir_name}/{student_id}_hw7_epoch{num_epoch}_ratio{support_ratio}_size{data_size}.json\"\n",
        "output_list = []\n",
        "for data in test_data:\n",
        "  original_response = original_model_response.pop(0)\n",
        "  aligned_response = aligned_model_response.pop(0)\n",
        "  output_list.append({\"id\": data[\"id\"], \"prompt\": data[\"prompt\"], \"original_response\": original_response, \"aligned_response\": aligned_response})\n",
        "output_data = {\"num_epoch\": num_epoch, \"data_size\": data_size, \"support_ratio\": support_ratio, \"results\": output_list}\n",
        "with open(file_name, \"w\") as output_file:\n",
        "    json.dump(output_data, output_file, indent=4)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mYcnLdtR1heY"
      },
      "source": [
        "Finally, we provide code for free testing.\n",
        "\n",
        "You may freely adjust the system prompt, user prompt and generate settings here for model behavior observations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DkSSgCHL1j1T"
      },
      "outputs": [],
      "source": [
        "def make_prompt(system, prompt):\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": system},\n",
        "        {\"role\": \"user\", \"content\": prompt},\n",
        "    ]\n",
        "    prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "    return prompt\n",
        "\n",
        "# TODO: Try your system prompt and user prompt here.\n",
        "system = \"Your entire response must be 100 characters or less.\"\n",
        "prompt = \"\"\n",
        "\n",
        "inputs = make_prompt(system, prompt)\n",
        "outputs = model.generate(\n",
        "    **tokenizer(inputs, return_tensors = \"pt\").to(\"cuda\"),\n",
        "    max_new_tokens = 512, # TODO: You may use this for early stop.\n",
        "    do_sample=False, # Please keep this to False and do not tweak other parameters.\n",
        ")\n",
        "output = tokenizer.batch_decode(outputs)[0]\n",
        "output = extract_assistant_response(output)\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rJIeolv94KtF"
      },
      "source": [
        "And that's it for homework 7! If you have any questions, please consider posting questions in the discussion forum first so all the classmates can benefit. TAs will also prioritize responding to questions posted there.\n",
        "\n",
        "Also, please make sure that you have completed the submission for both GradeScope and NTU Cool.\n",
        "\n",
        "Good luck!\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}